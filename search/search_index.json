{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to documentation for IEETA HPC This documentation is designed to provide users with essential information and instructions to effectively utilize the HPC resources at IEETA. Whether you're new to HPC or an experienced user, this guide aims to help you understand how to access, manage jobs, and leverage the available resources in the most efficient way. Contents About : This section provides an overview of the IEETA HPC, including its purpose, capabilities, and the team responsible for its supports. How to Access : Detailed instructions on how to gain access to the IEETA HPC Quick start : A concise guide for quickly getting started with the IEETA HPC. This includes basic steps on logging in, running simple jobs, and understanding the file system. For users seeking more profound insights or for those new to this domain, we have curated additional detailed material that delves into various aspects of utilizing the IEETA HPC. Detailed Material : Software packages Job management Account management Furthermore, to aid in acclimatization with the HPC stack, we have also included some pragmatic examples: Deep Learning (GPU) Examples MNIST Example Scientific Computing (CPU) Examples DNA Sequencing Example R Example","title":"Home"},{"location":"#welcome-to-documentation-for-ieeta-hpc","text":"This documentation is designed to provide users with essential information and instructions to effectively utilize the HPC resources at IEETA. Whether you're new to HPC or an experienced user, this guide aims to help you understand how to access, manage jobs, and leverage the available resources in the most efficient way.","title":"Welcome to documentation for IEETA HPC"},{"location":"#contents","text":"About : This section provides an overview of the IEETA HPC, including its purpose, capabilities, and the team responsible for its supports. How to Access : Detailed instructions on how to gain access to the IEETA HPC Quick start : A concise guide for quickly getting started with the IEETA HPC. This includes basic steps on logging in, running simple jobs, and understanding the file system. For users seeking more profound insights or for those new to this domain, we have curated additional detailed material that delves into various aspects of utilizing the IEETA HPC. Detailed Material : Software packages Job management Account management Furthermore, to aid in acclimatization with the HPC stack, we have also included some pragmatic examples: Deep Learning (GPU) Examples MNIST Example Scientific Computing (CPU) Examples DNA Sequencing Example R Example","title":"Contents"},{"location":"about/","text":"About IEETA HPC This page is dedicated to providing you with a comprehensive understanding of the IEETA HPC facility, its purpose, capabilities, and the unique opportunities it offers for research and computation-intensive projects. Objectives Support Advanced Research: The primary objective of IEETA HPC is to enable and accelerate research that requires substantial computational resources. Offer an open and unified computation platform: By aggregating heterogenous computation devices under the same cluster, we can facilitate in a unified way and fair access to all of our reshearchers. Capabilities IEETA HPC offers a range of computing resources, including traditional CPU clusters and GPU-enabled nodes for deep learning tasks. Cluster Configuration The IEETA HPC facility features a robust hardware configuration, including: Compute Nodes: TODO GPU Nodes: TODO Storage: TODO The management of access to the cluster is primarily conducted through Slurm . Support Team Currently the IEETA HPC is responsability of the \"Pelouro de Infraestrutra\" team: Jo\u00e3o Rodrigues (jmr@ua.pt) Eurico Pedrosa (efp@live.ua.pt) Tiago Almeida (tiagomeloalmeida@ua.pt)","title":"About"},{"location":"about/#about-ieeta-hpc","text":"This page is dedicated to providing you with a comprehensive understanding of the IEETA HPC facility, its purpose, capabilities, and the unique opportunities it offers for research and computation-intensive projects.","title":"About IEETA HPC"},{"location":"about/#objectives","text":"Support Advanced Research: The primary objective of IEETA HPC is to enable and accelerate research that requires substantial computational resources. Offer an open and unified computation platform: By aggregating heterogenous computation devices under the same cluster, we can facilitate in a unified way and fair access to all of our reshearchers.","title":"Objectives"},{"location":"about/#capabilities","text":"IEETA HPC offers a range of computing resources, including traditional CPU clusters and GPU-enabled nodes for deep learning tasks.","title":"Capabilities"},{"location":"about/#cluster-configuration","text":"The IEETA HPC facility features a robust hardware configuration, including: Compute Nodes: TODO GPU Nodes: TODO Storage: TODO The management of access to the cluster is primarily conducted through Slurm .","title":"Cluster Configuration"},{"location":"about/#support-team","text":"Currently the IEETA HPC is responsability of the \"Pelouro de Infraestrutra\" team: Jo\u00e3o Rodrigues (jmr@ua.pt) Eurico Pedrosa (efp@live.ua.pt) Tiago Almeida (tiagomeloalmeida@ua.pt)","title":"Support Team"},{"location":"how_to_access/","text":"How to Access Overview The IEETA HPC facility primarily serves researchers at IEETA and students of the University of Aveiro. This guide outlines the process for obtaining access to the HPC resources and the method for connecting to the system. Eligibility and Requesting Access Who Can Access Researchers at IEETA: Faculty members, postdoctoral researchers, and research staff affiliated with IEETA are eligible to access the HPC resources. Students at the University of Aveiro: Undergraduate and graduate students enrolled at the University of Aveiro with IEETA supervisors are eligible to use the HPC for academic and research purposes. How to Request Access To request access to the IEETA HPC, eligible users should fill the following form https://forms.gle/WvRmjL1krNykzLnX7 Connecting to the HPC Once your access is granted, you can connect to the HPC using SSH (Secure Shell) with your universal University of Aveiro user credentials. Note: Access to the IEETA HPC (hpc.ieeta.pt) is restricted to the University of Aveiro network, including Eduroam. Ensure you are connected to this network for successful access. SSH Connection Steps Open a Terminal: On your local machine, open a terminal window. Initiate SSH Connection: Use the following SSH command, replacing YourUniversityUsername with your actual username: ssh YourUniversityUsername@hpc.ieeta.pt Enter Your Password: When prompted, enter your University of Aveiro password. Further Assistance If you encounter any issues or have queries regarding the access process, please feel free to reach out to the HPC support team for assistance.","title":"How to Access"},{"location":"how_to_access/#how-to-access","text":"","title":"How to Access"},{"location":"how_to_access/#overview","text":"The IEETA HPC facility primarily serves researchers at IEETA and students of the University of Aveiro. This guide outlines the process for obtaining access to the HPC resources and the method for connecting to the system.","title":"Overview"},{"location":"how_to_access/#eligibility-and-requesting-access","text":"","title":"Eligibility and Requesting Access"},{"location":"how_to_access/#who-can-access","text":"Researchers at IEETA: Faculty members, postdoctoral researchers, and research staff affiliated with IEETA are eligible to access the HPC resources. Students at the University of Aveiro: Undergraduate and graduate students enrolled at the University of Aveiro with IEETA supervisors are eligible to use the HPC for academic and research purposes.","title":"Who Can Access"},{"location":"how_to_access/#how-to-request-access","text":"To request access to the IEETA HPC, eligible users should fill the following form https://forms.gle/WvRmjL1krNykzLnX7","title":"How to Request Access"},{"location":"how_to_access/#connecting-to-the-hpc","text":"Once your access is granted, you can connect to the HPC using SSH (Secure Shell) with your universal University of Aveiro user credentials. Note: Access to the IEETA HPC (hpc.ieeta.pt) is restricted to the University of Aveiro network, including Eduroam. Ensure you are connected to this network for successful access.","title":"Connecting to the HPC"},{"location":"how_to_access/#ssh-connection-steps","text":"Open a Terminal: On your local machine, open a terminal window. Initiate SSH Connection: Use the following SSH command, replacing YourUniversityUsername with your actual username: ssh YourUniversityUsername@hpc.ieeta.pt Enter Your Password: When prompted, enter your University of Aveiro password.","title":"SSH Connection Steps"},{"location":"how_to_access/#further-assistance","text":"If you encounter any issues or have queries regarding the access process, please feel free to reach out to the HPC support team for assistance.","title":"Further Assistance"},{"location":"quick_start/","text":"Quick start In this page you can find a quick start guide on how to use IEETA cluster (Pleiades). 1. Access IEETA cluster (Pleiades) Access the cluster via SSH using the credentials provided to you by email. If you do not have access yet, please refer to the how_to_access.md page. $ ssh user@pleiades.ieeta.pt By default, upon logging in, you will land on our login node in your home directory, which is located at /data/home . This is a network storage partition visible to all cluster nodes. The login node is where you should prepare your code in order to submit jobs to run on the worker nodes of the cluster. The worker nodes are equipped with powerful resources. Currently, we have: CPU nodes : Nodes with a high amount of RAM and faster CPUs. Currently not added to the cluster yet GPU nodes : Nodes equipped with GPUs and more modest CPU/RAM configurations. For more information about each node check the nodes page . 2. Prepare your software environment The next step is to prepare your environment to run/build your application. We recommend using a virtual environment so that you can install any package locally. First, load the Python module. $ module load python Then create and activate your virtual environment. $ python -m venv virtual-venv $ source virtual-venv/bin/activate You can then install your package dependencies with pip. (virtual-venv)$ pip install --upgrade pip (virtual-venv)$ pip install torch transformers 3. Create your SLURM job script After setting up your runtime environment, you should create a SLURM job script to submit your job. For example: #!/bin/bash #SBATCH --job-name=trainer # create a short name for your job #SBATCH --output=\"trainer-%j.out\" # %j will be replaced by the slurm jobID #SBATCH --nodes=1 # node count #SBATCH --ntasks=1 # total number of tasks across all nodes #SBATCH --cpus-per-task=2 # cpu-cores per task (>1 if multi-threaded tasks) #SBATCH --gres=gpu:1 # number of gpus per node #SBATCH --mem=4G # Total amount of RAM requested source /virtual-venv/bin/activate # If you have your venv activated when you submit the job, then you do not need to activate/deactivate python your_trainer_script.py deactivate The script is made of two parts: 1. Specification of the resources needed and some job information; 2. Comands that will be executed on the destination node. As an example, in the first part of the script, we define the job name, the output file and the requested resources (1 GPU, 2 CPUs and 4GB RAM). Then, in the second part, we define the tasks of the job. By default since no partition was defined the job will run under the default partitaion that in this cluster is the gpu partition, you can check which partitions and nodes are available with: bash $ sinfo bash 4. Submit the job To submit the job, you should run the following command: ```bash $ sbatch script_trainer.sh Submitted batch job 144 You can check the job status using the following command: bash $ squeue bash","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"In this page you can find a quick start guide on how to use IEETA cluster (Pleiades).","title":"Quick start"},{"location":"quick_start/#1-access-ieeta-cluster-pleiades","text":"Access the cluster via SSH using the credentials provided to you by email. If you do not have access yet, please refer to the how_to_access.md page. $ ssh user@pleiades.ieeta.pt By default, upon logging in, you will land on our login node in your home directory, which is located at /data/home . This is a network storage partition visible to all cluster nodes. The login node is where you should prepare your code in order to submit jobs to run on the worker nodes of the cluster. The worker nodes are equipped with powerful resources. Currently, we have: CPU nodes : Nodes with a high amount of RAM and faster CPUs. Currently not added to the cluster yet GPU nodes : Nodes equipped with GPUs and more modest CPU/RAM configurations. For more information about each node check the nodes page .","title":"1. Access IEETA cluster (Pleiades)"},{"location":"quick_start/#2-prepare-your-software-environment","text":"The next step is to prepare your environment to run/build your application. We recommend using a virtual environment so that you can install any package locally. First, load the Python module. $ module load python Then create and activate your virtual environment. $ python -m venv virtual-venv $ source virtual-venv/bin/activate You can then install your package dependencies with pip. (virtual-venv)$ pip install --upgrade pip (virtual-venv)$ pip install torch transformers","title":"2. Prepare your software environment"},{"location":"quick_start/#3-create-your-slurm-job-script","text":"After setting up your runtime environment, you should create a SLURM job script to submit your job. For example: #!/bin/bash #SBATCH --job-name=trainer # create a short name for your job #SBATCH --output=\"trainer-%j.out\" # %j will be replaced by the slurm jobID #SBATCH --nodes=1 # node count #SBATCH --ntasks=1 # total number of tasks across all nodes #SBATCH --cpus-per-task=2 # cpu-cores per task (>1 if multi-threaded tasks) #SBATCH --gres=gpu:1 # number of gpus per node #SBATCH --mem=4G # Total amount of RAM requested source /virtual-venv/bin/activate # If you have your venv activated when you submit the job, then you do not need to activate/deactivate python your_trainer_script.py deactivate The script is made of two parts: 1. Specification of the resources needed and some job information; 2. Comands that will be executed on the destination node. As an example, in the first part of the script, we define the job name, the output file and the requested resources (1 GPU, 2 CPUs and 4GB RAM). Then, in the second part, we define the tasks of the job. By default since no partition was defined the job will run under the default partitaion that in this cluster is the gpu partition, you can check which partitions and nodes are available with: bash $ sinfo bash","title":"3. Create your SLURM job script"},{"location":"quick_start/#4-submit-the-job","text":"To submit the job, you should run the following command: ```bash $ sbatch script_trainer.sh Submitted batch job 144 You can check the job status using the following command: bash $ squeue bash","title":"4. Submit the job"},{"location":"detail_material/account_management/","text":"","title":"Account management"},{"location":"detail_material/job_management/","text":"","title":"Job management"},{"location":"detail_material/nodes/","text":"","title":"Nodes"},{"location":"detail_material/software_packages/","text":"","title":"Software packages"},{"location":"examples/dl/mnist/","text":"","title":"MNIST example"},{"location":"examples/dl/transformers/","text":"","title":"Transformers example"},{"location":"examples/sc/dna/","text":"","title":"DNA sequencing"},{"location":"examples/sc/r/","text":"","title":"R"}]}