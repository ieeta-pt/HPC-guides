{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Pleiades documentation This documentation is designed to provide users with essential information and instructions to effectively utilize the resources at IEETA. Whether you're new to HPC or an experienced user, this guide aims to help you understand how to access, manage jobs, and leverage the available resources in the most efficient way. Contents About : This section provides an overview of Pleiades (IEETA's HPC cluster), including its purpose, capabilities, and the team responsible for its supports. How to Access : Detailed instructions on how to gain access to Pleiades Quick start : A concise guide for quickly getting started with the Pleiades. This includes basic steps on logging in, running simple jobs, and understanding the file system. For users seeking more profound insights or for those new to this domain, we have curated additional detailed material that delves into various aspects of utilizing this cluster. Detailed Material : Cluster Nodes Software packages Job management Account management Furthermore, to aid in acclimatization with the HPC stack, we have also included some practical examples: Deep Learning (GPU) Examples Transformers Cuda example MNIST Example Scientific Computing (CPU) Examples DNA Sequencing Example R Example","title":"Home"},{"location":"#welcome-to-the-pleiades-documentation","text":"This documentation is designed to provide users with essential information and instructions to effectively utilize the resources at IEETA. Whether you're new to HPC or an experienced user, this guide aims to help you understand how to access, manage jobs, and leverage the available resources in the most efficient way.","title":"Welcome to the Pleiades documentation"},{"location":"#contents","text":"About : This section provides an overview of Pleiades (IEETA's HPC cluster), including its purpose, capabilities, and the team responsible for its supports. How to Access : Detailed instructions on how to gain access to Pleiades Quick start : A concise guide for quickly getting started with the Pleiades. This includes basic steps on logging in, running simple jobs, and understanding the file system. For users seeking more profound insights or for those new to this domain, we have curated additional detailed material that delves into various aspects of utilizing this cluster. Detailed Material : Cluster Nodes Software packages Job management Account management Furthermore, to aid in acclimatization with the HPC stack, we have also included some practical examples: Deep Learning (GPU) Examples Transformers Cuda example MNIST Example Scientific Computing (CPU) Examples DNA Sequencing Example R Example","title":"Contents"},{"location":"about/","text":"About Pleiades This page is dedicated to providing a comprehensive understanding of Pleiades, the name of IEETA's HPC cluster. Here, you will learn about its purpose, capabilities, and the unique opportunities it offers for research and computation-intensive projects. Objectives Support Advanced Research: The primary objective of Pleiades is to enable and accelerate research that requires substantial computational resources. Offer an open and unified computation platform: By aggregating heterogenous computation devices under the same cluster, we can facilitate in a unified way fair access to all of our reshearchers. Capabilities Pleiades offers a range of computing resources, including traditional CPU and GPU -enabled nodes. Why this name (TODO: short description of the name) Support Team Currently the IEETA HPC is responsability of the \"Pelouro de Infraestrutra\" team: Jo\u00e3o Rodrigues (jmr@ua.pt) Eurico Pedrosa (efp@live.ua.pt) Tiago Almeida (tiagomeloalmeida@ua.pt)","title":"About"},{"location":"about/#about-pleiades","text":"This page is dedicated to providing a comprehensive understanding of Pleiades, the name of IEETA's HPC cluster. Here, you will learn about its purpose, capabilities, and the unique opportunities it offers for research and computation-intensive projects.","title":"About Pleiades"},{"location":"about/#objectives","text":"Support Advanced Research: The primary objective of Pleiades is to enable and accelerate research that requires substantial computational resources. Offer an open and unified computation platform: By aggregating heterogenous computation devices under the same cluster, we can facilitate in a unified way fair access to all of our reshearchers.","title":"Objectives"},{"location":"about/#capabilities","text":"Pleiades offers a range of computing resources, including traditional CPU and GPU -enabled nodes.","title":"Capabilities"},{"location":"about/#why-this-name","text":"(TODO: short description of the name)","title":"Why this name"},{"location":"about/#support-team","text":"Currently the IEETA HPC is responsability of the \"Pelouro de Infraestrutra\" team: Jo\u00e3o Rodrigues (jmr@ua.pt) Eurico Pedrosa (efp@live.ua.pt) Tiago Almeida (tiagomeloalmeida@ua.pt)","title":"Support Team"},{"location":"how_to_access/","text":"How to Access Overview Pleiades primarily serves researchers at IEETA and students of the University of Aveiro. This guide outlines the process for obtaining access and connecting to the cluster. Eligibility and Requesting Access Who Can Access Researchers at IEETA: Faculty members, postdoctoral researchers, and research staff affiliated with IEETA are eligible to access IEETA resources. Students at the University of Aveiro: Undergraduate and graduate students enrolled at the University of Aveiro with IEETA supervisors are eligible to use these computational facility for academic and research purposes. How to Request Access Currently, to request access to Pleiades, eligible users should fill the following form https://forms.gle/WvRmjL1krNykzLnX7 Connecting to the HPC Once your access is granted, you can connect to Pleiades using SSH (Secure Shell) with your user credentials. Note: Access to the Pleiades (pleiades.ieeta.pt) is restricted to the University of Aveiro network, including Eduroam. Ensure you are connected to this network for successful access. SSH Connection Steps Open a Terminal: On your local machine, open a terminal window. Initiate SSH Connection: Use the following SSH command, replacing username with the username provided to you by email: ssh username@pleiades.ieeta.pt Enter Your Password: When prompted, enter the password provided to you. Further Assistance If you encounter any issues or have queries regarding the access process, please feel free to reach out to the Pleiades support team for assistance.","title":"How to Access"},{"location":"how_to_access/#how-to-access","text":"","title":"How to Access"},{"location":"how_to_access/#overview","text":"Pleiades primarily serves researchers at IEETA and students of the University of Aveiro. This guide outlines the process for obtaining access and connecting to the cluster.","title":"Overview"},{"location":"how_to_access/#eligibility-and-requesting-access","text":"","title":"Eligibility and Requesting Access"},{"location":"how_to_access/#who-can-access","text":"Researchers at IEETA: Faculty members, postdoctoral researchers, and research staff affiliated with IEETA are eligible to access IEETA resources. Students at the University of Aveiro: Undergraduate and graduate students enrolled at the University of Aveiro with IEETA supervisors are eligible to use these computational facility for academic and research purposes.","title":"Who Can Access"},{"location":"how_to_access/#how-to-request-access","text":"Currently, to request access to Pleiades, eligible users should fill the following form https://forms.gle/WvRmjL1krNykzLnX7","title":"How to Request Access"},{"location":"how_to_access/#connecting-to-the-hpc","text":"Once your access is granted, you can connect to Pleiades using SSH (Secure Shell) with your user credentials. Note: Access to the Pleiades (pleiades.ieeta.pt) is restricted to the University of Aveiro network, including Eduroam. Ensure you are connected to this network for successful access.","title":"Connecting to the HPC"},{"location":"how_to_access/#ssh-connection-steps","text":"Open a Terminal: On your local machine, open a terminal window. Initiate SSH Connection: Use the following SSH command, replacing username with the username provided to you by email: ssh username@pleiades.ieeta.pt Enter Your Password: When prompted, enter the password provided to you.","title":"SSH Connection Steps"},{"location":"how_to_access/#further-assistance","text":"If you encounter any issues or have queries regarding the access process, please feel free to reach out to the Pleiades support team for assistance.","title":"Further Assistance"},{"location":"quick_start/","text":"Quick start In this page you can find a quick start guide on how to use IEETA cluster (Pleiades). 1. Access IEETA cluster (Pleiades) Access the cluster via SSH using the credentials provided to you by email. If you do not have access yet, please refer to the How to Access page. $ ssh user@pleiades.ieeta.pt By default, upon logging in, you will land on our login node in your home directory, which is located at /data/home . This is a network storage partition visible to all cluster nodes. The login node is where you should prepare your code in order to submit jobs to run on the worker nodes of the cluster. The worker nodes are equipped with powerful resources. Currently, we have: CPU nodes : Nodes with a high amount of RAM and faster CPUs. Currently not added to the cluster yet GPU nodes : Nodes equipped with GPUs and more modest CPU/RAM configurations. For more information about each node check the nodes page . 2. Prepare your software environment The next step is to prepare your environment to run/build your application. We recommend using a virtual environment so that you can install any package locally. First, load the Python module. $ module load python Then create and activate your virtual environment. $ python -m venv virtual-venv $ source virtual-venv/bin/activate You can then install your package dependencies with pip. (virtual-venv)$ pip install --upgrade pip (virtual-venv)$ pip install torch transformers 3. Create your SLURM job script After setting up your runtime environment, you should create a SLURM job script to submit your job. For example: #!/bin/bash #SBATCH --job-name=trainer # create a short name for your job #SBATCH --output=\"trainer-%j.out\" # %j will be replaced by the slurm jobID #SBATCH --nodes=1 # node count #SBATCH --ntasks=1 # total number of tasks across all nodes #SBATCH --cpus-per-task=2 # cpu-cores per task (>1 if multi-threaded tasks) #SBATCH --gres=gpu:1 # number of gpus per node #SBATCH --mem=4G # Total amount of RAM requested source /virtual-venv/bin/activate # If you have your venv activated when you submit the job, then you do not need to activate/deactivate python your_trainer_script.py deactivate The script is made of two parts: 1. Specification of the resources needed and some job information; 2. Comands that will be executed on the destination node. As an example, in the first part of the script, we define the job name, the output file and the requested resources (1 GPU, 2 CPUs and 4GB RAM). Then, in the second part, we define the tasks of the job. By default since no partition was defined the job will run under the default partitaion that in this cluster is the gpu partition, you can check which partitions and nodes are available with: $ sinfo 4. Submit the job To submit the job, you should run the following command: $ sbatch script_trainer.sh Submitted batch job 144 You can check the job status using the following command: $ squeue","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"In this page you can find a quick start guide on how to use IEETA cluster (Pleiades).","title":"Quick start"},{"location":"quick_start/#1-access-ieeta-cluster-pleiades","text":"Access the cluster via SSH using the credentials provided to you by email. If you do not have access yet, please refer to the How to Access page. $ ssh user@pleiades.ieeta.pt By default, upon logging in, you will land on our login node in your home directory, which is located at /data/home . This is a network storage partition visible to all cluster nodes. The login node is where you should prepare your code in order to submit jobs to run on the worker nodes of the cluster. The worker nodes are equipped with powerful resources. Currently, we have: CPU nodes : Nodes with a high amount of RAM and faster CPUs. Currently not added to the cluster yet GPU nodes : Nodes equipped with GPUs and more modest CPU/RAM configurations. For more information about each node check the nodes page .","title":"1. Access IEETA cluster (Pleiades)"},{"location":"quick_start/#2-prepare-your-software-environment","text":"The next step is to prepare your environment to run/build your application. We recommend using a virtual environment so that you can install any package locally. First, load the Python module. $ module load python Then create and activate your virtual environment. $ python -m venv virtual-venv $ source virtual-venv/bin/activate You can then install your package dependencies with pip. (virtual-venv)$ pip install --upgrade pip (virtual-venv)$ pip install torch transformers","title":"2. Prepare your software environment"},{"location":"quick_start/#3-create-your-slurm-job-script","text":"After setting up your runtime environment, you should create a SLURM job script to submit your job. For example: #!/bin/bash #SBATCH --job-name=trainer # create a short name for your job #SBATCH --output=\"trainer-%j.out\" # %j will be replaced by the slurm jobID #SBATCH --nodes=1 # node count #SBATCH --ntasks=1 # total number of tasks across all nodes #SBATCH --cpus-per-task=2 # cpu-cores per task (>1 if multi-threaded tasks) #SBATCH --gres=gpu:1 # number of gpus per node #SBATCH --mem=4G # Total amount of RAM requested source /virtual-venv/bin/activate # If you have your venv activated when you submit the job, then you do not need to activate/deactivate python your_trainer_script.py deactivate The script is made of two parts: 1. Specification of the resources needed and some job information; 2. Comands that will be executed on the destination node. As an example, in the first part of the script, we define the job name, the output file and the requested resources (1 GPU, 2 CPUs and 4GB RAM). Then, in the second part, we define the tasks of the job. By default since no partition was defined the job will run under the default partitaion that in this cluster is the gpu partition, you can check which partitions and nodes are available with: $ sinfo","title":"3. Create your SLURM job script"},{"location":"quick_start/#4-submit-the-job","text":"To submit the job, you should run the following command: $ sbatch script_trainer.sh Submitted batch job 144 You can check the job status using the following command: $ squeue","title":"4. Submit the job"},{"location":"detail_material/account_management/","text":"","title":"Account management"},{"location":"detail_material/job_management/","text":"","title":"Job management"},{"location":"detail_material/nodes/","text":"Cluster Nodes We are currently in a testing phase, and nodes will be gradually added or migrated to the cluster. GPU Nodes: dl-srv-02: Type: HP-workstation CPU: i7-12700 (12C) RAM: 16GB GPU-0: A2000 Temporal local storage: 512GB nvme (/tmp/your-job) Slurm GPU resource name: nvidia-rtx-a2000 dl-srv-03: Type: Asus Server CPU: EPYC 7543 (32C/64T) RAM: 256GB GPU-0: A6000 GPU-1: A6000 Temporal local storage: 512GB nvme (/tmp/your-job) Slurm GPU resource name: nvidia-rtx-a6000 (More to be added in the future) CPU Nodes: (More to be added in the future) Login Node: Type: VM CPU: 22 Cores RAM: 66GB","title":"Cluster Nodes"},{"location":"detail_material/nodes/#cluster-nodes","text":"We are currently in a testing phase, and nodes will be gradually added or migrated to the cluster.","title":"Cluster Nodes"},{"location":"detail_material/nodes/#gpu-nodes","text":"","title":"GPU Nodes:"},{"location":"detail_material/nodes/#dl-srv-02","text":"Type: HP-workstation CPU: i7-12700 (12C) RAM: 16GB GPU-0: A2000 Temporal local storage: 512GB nvme (/tmp/your-job) Slurm GPU resource name: nvidia-rtx-a2000","title":"dl-srv-02:"},{"location":"detail_material/nodes/#dl-srv-03","text":"Type: Asus Server CPU: EPYC 7543 (32C/64T) RAM: 256GB GPU-0: A6000 GPU-1: A6000 Temporal local storage: 512GB nvme (/tmp/your-job) Slurm GPU resource name: nvidia-rtx-a6000 (More to be added in the future)","title":"dl-srv-03:"},{"location":"detail_material/nodes/#cpu-nodes","text":"(More to be added in the future)","title":"CPU Nodes:"},{"location":"detail_material/nodes/#login-node","text":"Type: VM CPU: 22 Cores RAM: 66GB","title":"Login Node:"},{"location":"detail_material/software_packages/","text":"","title":"Software packages"},{"location":"examples/dl/mnist/","text":"","title":"MNIST example"},{"location":"examples/dl/transformers/","text":"","title":"Transformers example"},{"location":"examples/sc/dna/","text":"","title":"DNA sequencing"},{"location":"examples/sc/r/","text":"","title":"R"}]}